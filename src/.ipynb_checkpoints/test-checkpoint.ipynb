{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Delish Webscraper Version1.\n",
    "\"\"\"\n",
    "import csv\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "#get current time\n",
    "now = datetime.datetime.now()   \n",
    "    \n",
    "def parse_string(string_):\n",
    "    string_ = string_.replace('\\t', '').replace(\"  \", \"\").replace('\\n', \"\")\n",
    "    return string_\n",
    "def parse_list(string_list):\n",
    "    #create a list of the Items captured by the Beautiful Soup.get_text removes the tages but not the formating,\n",
    "    # so the replace functions are taking out \\t, \\n, \\xa0 formating\n",
    "    new_list=[item.get_text().replace('\\t', '').replace(\"  \", \"\").replace('\\n', \"\").replace('\\xa0',\"\") for item in string_list]\n",
    "    return new_list\n",
    "def get_data_allrecipes(url_string):\n",
    "    \n",
    "    try:\n",
    "        uClient = uReq(url_string)\n",
    "        page_html = uClient.read()\n",
    "        uClient.close()\n",
    "    #html parsing\n",
    "        page_soup = soup(page_html ,\"html.parser\")\n",
    "    #url\n",
    "        url = url_string\n",
    "        print(url)\n",
    "    #title\n",
    "        title= page_soup.find('h1').get_text()\n",
    "        print(title)\n",
    "        prep_time = page_soup.find('span', {'class':'prep-time-amount'}).get_text()\n",
    "        prep_time = parse_string(prep_time)\n",
    "        print(prep_time)\n",
    "    #time\n",
    "        total_time = page_soup.find('span', {'class':\"total-time-amount\"}).get_text()\n",
    "        total_time = parse_string(total_time)\n",
    "        print(total_time)\n",
    "        #ingredients\n",
    "        ingredient_amount = page_soup.findAll('span',{'class':'ingredient-amount'})\n",
    "        ingredient_amount_list = parse_list(ingredient_amount)\n",
    "        #print(ingredient_amount_list)\n",
    "        ingredients = page_soup.findAll('span',{'class':'ingredient-description'})\n",
    "        ingredient_list =parse_list(ingredients)\n",
    "        ingredients_ = [str(m)+n for m,n in zip(ingredient_amount_list,ingredient_list)] \n",
    "        print(ingredients_)\n",
    "    #instructions\n",
    "        instructions = page_soup.findAll('div',{'class':\"direction-lists\"})\n",
    "        instruction_list = parse_list(instructions)\n",
    "        print(instruction_list)\n",
    "        scraperdata= url, title, prep_time, total_time, ingredients_,instruction_list\n",
    "        return scraperdata\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "proteins= [\"beef\", \"chicken\", \"shrimp\", \"lentils\"]\n",
    "website_string = \"http://www.delish.com/search/\"\n",
    "appended_lists=[]\n",
    "file_name = now.isoformat()+\".csv\"\n",
    "file_name = file_name.replace(\":\", \"_\")\n",
    "with open(file_name, \"w\",newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(\"URL\", \"Title\", \"Prep Time\", \"Total Time\", \"Ingredients\",\"Instructions\")\n",
    "        \n",
    "    for protein in proteins:\n",
    "        search_string= website_string+protein\n",
    "        website_page = uReq(search_string)\n",
    "        page_soup = soup(website_page, \"html.parser\")\n",
    "        for link in page_soup.findAll('a', href=re.compile(\"/cooking/recipe-ideas\")):\n",
    "            url_string = website_string+link['href']\n",
    "            scraper_content_list = get_data_allrecipes(url_string)\n",
    "            appended_lists.append(scraper_content_list)\n",
    "            writer.writerow([scraper_content_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "connection = sqlite3.connect(host='localhost',\n",
    "                             user='root',\n",
    "                             password='password',\n",
    "                             db='recipes',\n",
    "                             charset='utf8mb4',\n",
    "                             cursorclass=pymysql.cursors.DictCursor)\n",
    " \n",
    "try:\n",
    "    with connection.cursor() as cursor:\n",
    "        for record in recordList:\n",
    "                for protein in proteins:\n",
    "                    search_string= website_string+protein\n",
    "                    website_page = uReq(search_string)\n",
    "                    page_soup = soup(website_page, \"html.parser\")\n",
    "                    for link in page_soup.findAll('a', href=re.compile(\"/cooking/recipe-ideas\")):\n",
    "                        url_string = website_string+link['href']\n",
    "                        scraper_content_list = get_data_allrecipes(url_string)\n",
    "                        appended_lists.append(scraper_content_list)\n",
    "                        writer.writerow([scraper_content_list])\n",
    "                        \n",
    "\n",
    "            sql = \"INSERT INTO `recipes`, (`search_string`,'prep_time','total_time', `website_page`,'instruction_list', 'ingredient_list') VALUES (%s, %s, s%, %s, %s, s%)\"\n",
    "            cursor.execute(sql, (search_string, title))\n",
    "            #goal is to move to something like this\n",
    "           # for row in c.execute('SELECT * FROM stocks ORDER BY price'):\n",
    "            #    print row\n",
    "    connection.commit()\n",
    "finally:\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
