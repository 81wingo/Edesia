{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Delish Webscraper Version1.\n",
    "\"\"\"\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "HEADER = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "}\n",
    "COOKIES = {\n",
    "    'euConsentFailed': 'true',\n",
    "    'euConsentID': 'e48da782-e1d1-0931-8796-d75863cdfa15',\n",
    "}\n",
    "\n",
    "scraper_content_list=[]\n",
    "\n",
    "def store_data(scraper_data):\n",
    "    df = pd.DataFrame(scraper_data)\n",
    "    file_name = now.isoformat()+\".csv\"\n",
    "    file_name = file_name.replace(\":\", \"_\")\n",
    "    df.to_csv(file_name)\n",
    "    print(\"file created\")\n",
    "    \n",
    "def parse_list(string_list):\n",
    "    #create a list of the Items captured by the Beautiful Soup.get_text removes the tages but not the formating,\n",
    "    # so the replace functions are taking out \\t, \\n, \\xa0 formating\n",
    "    new_list=[item.get_text().replace('\\t', '').replace(\"  \", \"\").replace('\\n', \"\").replace('\\xa0',\"\") for item in string_list]\n",
    "    return new_list\n",
    "def get_data_allrecipes(url_string):\n",
    "    scraper_content_list= []\n",
    "    scraper_content={}\n",
    "    try:\n",
    "        uClient = uReq(url_string)\n",
    "        page_html = uClient.read()\n",
    "        uClient.close()\n",
    "    #r = requests.get(page_html).content\n",
    "    #html parsing\n",
    "        page_soup = soup(page_html ,\"html.parser\")\n",
    "    #url\n",
    "        scraper_content['url'] = url_string\n",
    "    #title\n",
    "        scraper_content['title'] = page_soup.find('h1').get_text()\n",
    "    #time\n",
    "        scraper_content['time'] =page_soup.find('span', {'class':\"ready-in-time\"}) \n",
    "        ingredients = page_soup.findAll( 'li',{'class':\"checkList__line\"})\n",
    "    #ingredients\n",
    "        scraper_content['ingredient_list']=parse_list(ingredients)\n",
    "    #instructions\n",
    "        instructions = page_soup.findAll('span',{'class':\"recipe-directions_list--item\"})\n",
    "        scraper_content['instruction_list'] = parse_list(instruction)\n",
    "        scraper_content_list.append(scraper_content)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return scraper_content_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file created\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "proteins= [\"beef\", \"chicken\", \"shrimp\", \"lentils\"]\n",
    "website_string = \"http://www.delish.com/search/\"\n",
    "for protein in proteins:\n",
    "    search_string= website_string+protein\n",
    "    website_page = uReq(search_string)\n",
    "    page_soup = soup(website_page, \"html.parser\")\n",
    "    recipe_urls= []\n",
    "        \n",
    "        \n",
    "    for link in page_soup.findAll('a', href=re.compile(\"/cooking/recipe-ideas\")):\n",
    "        url_string = website_string+link['href']\n",
    "        #recipe_urls.append(recipe_url)\n",
    "        scraper_content_list = get_data_allrecipes(url_string)\n",
    "        \n",
    "store_data(scraper_content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper_content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
